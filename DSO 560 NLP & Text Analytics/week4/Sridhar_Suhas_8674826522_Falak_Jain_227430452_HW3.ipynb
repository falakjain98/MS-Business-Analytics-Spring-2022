{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zkn2hMRcDQVG"
   },
   "source": [
    "# HW3\n",
    "\n",
    "Submit via Slack. Due on **Tuesday, April 12th, 2022, 6:29pm PST**. You may work with one other person.\n",
    "## TF-IDF (5pts)\n",
    "\n",
    "You are an analyst working for Amazon's product team, and charged with identifying areas for improvement for the toy reviews.\n",
    "\n",
    "Using the **amazon-fine-foods.csv** dataset, clean and parse the text reviews. Explain the decisions you make:\n",
    "- why remove/keep stopwords?\n",
    "- which stopwords to remove?\n",
    "- stemming versus lemmatization?\n",
    "- regex cleaning and substitution?\n",
    "- adding in custom stopwords?\n",
    "- what `n` for your `n-grams`?\n",
    "\n",
    "Finally, generate a TF-IDF report that explains for a business (non-technical) stakeholder:\n",
    "* the features your analysis showed that customers cited as reasons for a poor review\n",
    "* the features your analysis showed that customers cited as reasons for a good review\n",
    "* the most common issues identified from your analysis that generated customer dissatisfaction.\n",
    "\n",
    "Explain to what degree the TF-IDF findings make sense - what are its limitations?\n",
    "\n",
    "\n",
    "## Similarity and Word Embeddings (2 pts)\n",
    "\n",
    "Using\n",
    "* `TfIdfVectorizer`\n",
    "\n",
    "Identify the most similar pair of reviews from the `amazon-fine-foods.csv` dataset using both Euclidean distance and cosine similarity.\n",
    "\n",
    "## Naive Bayes (3pts)\n",
    "\n",
    "You are an NLP data scientist working at Fandango. You observe the following dataset in your review comments:\n",
    "\n",
    "**Intent to Buy Tickets:**\n",
    "1.\tLove this movie. Canâ€™t wait!\n",
    "2.\tI want to see this movie so bad.\n",
    "3.\tThis movie looks amazing.\n",
    "\n",
    "**No Intent to Buy Tickets:**\n",
    "1.\tLooks bad.\n",
    "2.\tHard pass to see this bad movie.\n",
    "3.\tSo boring!\n",
    "\n",
    "You can consider the following stopwords for removal: `to`, `this`.\n",
    "\n",
    "Is the following review an `Intent to Buy` or `No Intent to Buy`? Show your work for each computation.\n",
    "> This looks so bad.\n",
    "\n",
    "You'll need to compute:\n",
    "* Prior\n",
    "* Likelihood\n",
    "* Posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "ok",
     "timestamp": 1649608421706,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "DXM9kJMYv_si",
    "outputId": "8c22845a-3b44-4105-dc16-9e451c148f80"
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import nltk\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1649608422132,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "fwGUHVi-FOag"
   },
   "outputs": [],
   "source": [
    "# defining the required functions\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbrBGG_LDyVL"
   },
   "source": [
    "# 1. TF-IDF (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1649608422135,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "Ld06Q5lbDsvM",
    "outputId": "fdee940f-f7b7-4cac-9ff9-e6b778504f1f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20983</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A21U4DR8M6I9QN</td>\n",
       "      <td>K. M Merrill \"justine\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318896000</td>\n",
       "      <td>addictive! but works for night coughing in dogs</td>\n",
       "      <td>my 12 year old sheltie has chronic brochotitis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20984</td>\n",
       "      <td>B002QWP89S</td>\n",
       "      <td>A17TDUBB4Z1PEC</td>\n",
       "      <td>jaded_green</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1318550400</td>\n",
       "      <td>genuine Greenies best price</td>\n",
       "      <td>These are genuine Greenies product, not a knoc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Id   ProductId          UserId             ProfileName  \\\n",
       "0  20983  B002QWP89S  A21U4DR8M6I9QN  K. M Merrill \"justine\"   \n",
       "1  20984  B002QWP89S  A17TDUBB4Z1PEC             jaded_green   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1318896000   \n",
       "1                     1                       1      5  1318550400   \n",
       "\n",
       "                                           Summary  \\\n",
       "0  addictive! but works for night coughing in dogs   \n",
       "1                      genuine Greenies best price   \n",
       "\n",
       "                                                Text  \n",
       "0  my 12 year old sheltie has chronic brochotitis...  \n",
       "1  These are genuine Greenies product, not a knoc...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11903, 10)\n"
     ]
    }
   ],
   "source": [
    "# Reading data\n",
    "data = pd.read_csv('../datasets/amazon_fine_foods.csv')\n",
    "display(data.head(2))\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-0aWxZoz7Ch"
   },
   "source": [
    "Products filtered:\n",
    "* B002QWP89S - Greenie dog treats\n",
    "* B0013NUGDE - Popchips potato chips\n",
    "* B007JFMH8M - Quaker soft baked oatmeal cookies\n",
    "* B000KV61FC - Tug a jug meal dispensing dog toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1649608422137,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "D4m6Hh920p_T"
   },
   "outputs": [],
   "source": [
    "# Filtering 4 products for analysis\n",
    "searchfor = ['B002QWP89S','B0013NUGDE','B007JFMH8M','B000KV61FC']\n",
    "data = data[data['ProductId'].str.contains('|'.join(searchfor))]\n",
    "data['Product'] = data['ProductId'].apply(lambda x: 'Greenie' if x == 'B002QWP89S'\n",
    "                                          else 'Popchips' if x == 'B0013NUGDE'\n",
    "                                          else 'Quaker_Oatmeal' if x == 'B007JFMH8M'\n",
    "                                          else 'Tug_a_jug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1649608422139,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "szHtpYoB4xgY"
   },
   "outputs": [],
   "source": [
    "# Removing columns which will not aid analysis and also dropping duplicates\n",
    "data.drop(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator','HelpfulnessDenominator', 'Time', 'Summary',],axis = 1, inplace = True)\n",
    "data.drop_duplicates('Text',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 475,
     "status": "ok",
     "timestamp": 1649608422590,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "xFRnc3d3EeWN",
    "outputId": "3cdc384d-48c7-4760-876d-ea48a59af61e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['the', 'i', 'and', 'a', 'to', 'it', 'of', 'my', 'for', 'is',\n",
       "       'they', 'are', 'this', 'in', 'these', 'that', 'but', 'them', 'was',\n",
       "       'have', 'with', 'so', 'not', 'on', 'dog', 'you', 'out', 'as', 'he',\n",
       "       'like', 'one', 'cookies', 'cookie', 'great', 'she', 'good', 'br',\n",
       "       'be', 'chips', 'get', 'love', 'her', 'at', 'soft', 'toy', 'just',\n",
       "       'very', 'would', 'if', 'we'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining word count\n",
    "data[\"Text\"] = data['Text'].str.lower().str.replace('[^\\w\\s]','')\n",
    "new_df = data.Text.str.split(expand=True).stack().value_counts().reset_index()\n",
    "new_df.columns = ['Word', 'Frequency']\n",
    "\n",
    "# Printing the most frequent words\n",
    "new_df['Word'][:50].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JbrLBqjjzDJW"
   },
   "source": [
    "- This shows that stopwords have to be removed as they are the most frequent and do not add value\n",
    "\n",
    "- We can go forward and remove the usual stopwords from the nltk stopwords along with the following: ('br','potato','quaker')\n",
    "\n",
    "- The following stopwords should be retained to determine sentiment and will be useful in ngrams: (\"not\",\"doesn't\",\"didn't\",\"very\",\"too\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1649608422592,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "tzT0fJiYHxgZ",
    "outputId": "dfa04ae7-9ac2-4e9c-9f80-e22e292539a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>12 year old sheltie chronic brochotitis meds t...</td>\n",
       "      <td>Greenie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>genuine greenies product not knockoff dogs lov...</td>\n",
       "      <td>Greenie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  Product\n",
       "0      5  12 year old sheltie chronic brochotitis meds t...  Greenie\n",
       "1      5  genuine greenies product not knockoff dogs lov...  Greenie"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing stopwords usign nltk\n",
    "# stop words to remove from default stopwords list and to add to list\n",
    "stops = [\"not\",\"doesn't\",\"didn't\",\"very\",\"too\"]\n",
    "to_add = [\"potato\",\"br\",\"quaker\",'influenster','mom','vox box','voxbox','oatmeal']\n",
    "# creating final list of stopwords\n",
    "my_stopwords = list(set(stopwords.words('english')) - set(stops) | set(to_add))\n",
    "\n",
    "#removing all stopwords from dataframe\n",
    "data['Text'] = data['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (my_stopwords)]))\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pqRpU3yi5nAZ"
   },
   "source": [
    "Upon analyzing the reviews, it appears that poor reviews usually have a score of 2 or less while good reviews have a score of 4 or more with a score of 3 usually corresponding to a neutral review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1649608422594,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "Yc3WR9WT43CT",
    "outputId": "30dca402-4c9c-4241-b7d6-e40e0b73f8f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Product</th>\n",
       "      <th>good_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>12 year old sheltie chronic brochotitis meds t...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>genuine greenies product not knockoff dogs lov...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  Product  \\\n",
       "0      5  12 year old sheltie chronic brochotitis meds t...  Greenie   \n",
       "1      5  genuine greenies product not knockoff dogs lov...  Greenie   \n",
       "\n",
       "   good_review  \n",
       "0            2  \n",
       "1            2  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorizing review as good or poor\n",
    "data['good_review'] = data['Score'].apply(lambda x: 2 if x > 3 else 0 if x < 3 else 1)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3uEvUl-7y7R"
   },
   "source": [
    "We should opt for lemmatization over stemming for the following reasons:\n",
    "- the corpus is smaller and can therefore handle the load of lemmatization\n",
    "- since we are looking for the reasons a customer leaves a good or poor review, the context of the words will also matter, which will also help reduce noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 340,
     "status": "ok",
     "timestamp": 1649608422916,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "AI3zP7cI6_bM"
   },
   "outputs": [],
   "source": [
    "# regex cleaning and substitution\n",
    "data['Text'] = data['Text'].str.lower() # converting to lower case\n",
    "data['Text'] = data['Text'].str.replace(r'[\\.]{2,}', ' ',regex = True) # more than 2 periods\n",
    "data['Text'] = data['Text'].str.replace(r'\\bteeth\\sclean\\b|\\bclean\\steeth\\b','cleanteeth',regex = True) # clean teeth\n",
    "data['Text'] = data['Text'].str.replace(r'\\bsour\\scream\\sonion\\b','sourcreamonion',regex = True) # sour cream flavor\n",
    "data['Text'] = data['Text'].str.replace(r'\\bsoft\\sbaked\\b','softbaked',regex = True) # soft bake oatmeal\n",
    "data['Text'] = data['Text'].str.replace(r'\\bcooki','',regex = True) # soft bake oatmeal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 7085,
     "status": "ok",
     "timestamp": 1649608429999,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "6BWXwzfvEVjW"
   },
   "outputs": [],
   "source": [
    "# stemming or lemmatization\n",
    "data['Text Lemmatized'] = data['Text'].apply(lemmatize_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1649608430001,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "0oSuYMzP9A13",
    "outputId": "6cd5843e-df77-4a13-f822-3fe2d9b5ba3c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of good reviews: 2154\n",
      "Total number of poor reviews: 280\n"
     ]
    }
   ],
   "source": [
    "# creating two different datasets for good and bad reviews\n",
    "data_good = data[data['good_review'] == 2]\n",
    "print(f'Total number of good reviews: {data_good.shape[0]}')\n",
    "data_poor = data[data['good_review'] == 0]\n",
    "print(f'Total number of poor reviews: {data_poor.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hdr7pPMnagQ2"
   },
   "source": [
    "We decided to use 3,4-grams in our analysis as we noticed that one or two words were usually just taken up by the product name or type and did not provide much idea about the customer sentiment about those products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1649608430003,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "4rH7zGKs4UYg",
    "outputId": "f51cec0d-ad50-401b-9637-af96438595fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greenie  good size  (571, 5)\n",
      "Greenie  poor size  (38, 5)\n",
      "Popchips  good size  (457, 5)\n",
      "Popchips  poor size  (59, 5)\n",
      "Quaker_Oatmeal  good size  (838, 5)\n",
      "Quaker_Oatmeal  poor size  (16, 5)\n",
      "Tug_a_jug  good size  (288, 5)\n",
      "Tug_a_jug  poor size  (167, 5)\n"
     ]
    }
   ],
   "source": [
    "# creating separate dataframes for each product\n",
    "products = ['Greenie','Popchips','Quaker_Oatmeal','Tug_a_jug']\n",
    "good_dict = {}\n",
    "poor_dict = {}\n",
    "for p in products:\n",
    "    good_dict[p] = data_good[data_good['Product'] == p]\n",
    "    print(p,' good size ',good_dict[p].shape)\n",
    "    poor_dict[p] = data_poor[data_poor['Product'] == p]\n",
    "    print(p,' poor size ',poor_dict[p].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1649608430004,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "h7fxNxPG84rT"
   },
   "outputs": [],
   "source": [
    "# initializing tf-idf vectorization for good reviews\n",
    "vectorizer = TfidfVectorizer(ngram_range=(4,4),\n",
    "                             token_pattern=r'\\b[a-zA-Z_]{3,}\\b',\n",
    "                             max_df=0.2, max_features=200, stop_words=stopwords.words())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1649608430330,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "wXmLaAvS50FK",
    "outputId": "27d25021-f87b-4911-b2eb-b4075063dd4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for good reviews of product Greenie: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>keep teeth gum clean</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>teeth clean vet since</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazon absolute best price</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>make breath smell good</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love help keep cleanteeth</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old golden retriever</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>help keep cleanteeth breath</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treat love good teeth</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year vet say teeth</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love greenies help keep</th>\n",
       "      <td>1.707107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                score\n",
       "keep teeth gum clean         3.000000\n",
       "teeth clean vet since        2.000000\n",
       "amazon absolute best price   2.000000\n",
       "make breath smell good       2.000000\n",
       "love help keep cleanteeth    2.000000\n",
       "year old golden retriever    2.000000\n",
       "help keep cleanteeth breath  2.000000\n",
       "treat love good teeth        2.000000\n",
       "year vet say teeth           1.707107\n",
       "love greenies help keep      1.707107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for good reviews of product Popchips: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>love salt vinegar chip</th>\n",
       "      <td>3.675161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>best chip ive ever</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taste like diet food</th>\n",
       "      <td>2.379232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>single serve bag pack</th>\n",
       "      <td>2.379232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dont feel guilty eat</th>\n",
       "      <td>2.379232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>low fat low calorie</th>\n",
       "      <td>2.372876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>healthy alternative regular chip</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feel like eat real</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>look forward try flavor</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>like salt vinegar flavor</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     score\n",
       "love salt vinegar chip            3.675161\n",
       "best chip ive ever                3.000000\n",
       "taste like diet food              2.379232\n",
       "single serve bag pack             2.379232\n",
       "dont feel guilty eat              2.379232\n",
       "low fat low calorie               2.372876\n",
       "healthy alternative regular chip  2.000000\n",
       "feel like eat real                2.000000\n",
       "look forward try flavor           2.000000\n",
       "like salt vinegar flavor          2.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for good reviews of product Quaker_Oatmeal: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>make whole grain oat</th>\n",
       "      <td>8.112036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cant wait try flavor</th>\n",
       "      <td>4.347384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft chewy taste like</th>\n",
       "      <td>3.554868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>soft chewy taste great</th>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>delicious definitely recommend everyone</th>\n",
       "      <td>2.691264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enough satisfy sweet tooth</th>\n",
       "      <td>2.669592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taste like home make</th>\n",
       "      <td>2.575090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>taste like home bake</th>\n",
       "      <td>2.398371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>individually packed make easy</th>\n",
       "      <td>2.251557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receive free sample softbaked</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            score\n",
       "make whole grain oat                     8.112036\n",
       "cant wait try flavor                     4.347384\n",
       "soft chewy taste like                    3.554868\n",
       "soft chewy taste great                   3.000000\n",
       "delicious definitely recommend everyone  2.691264\n",
       "enough satisfy sweet tooth               2.669592\n",
       "taste like home make                     2.575090\n",
       "taste like home bake                     2.398371\n",
       "individually packed make easy            2.251557\n",
       "receive free sample softbaked            2.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for good reviews of product Tug_a_jug: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>span classtiny length minsbr</th>\n",
       "      <td>4.137699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keep busy long time</th>\n",
       "      <td>2.407807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keep busy hour try</th>\n",
       "      <td>2.350411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year old border collie</th>\n",
       "      <td>2.192329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>get food keep entertain</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year use every day</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doesnt show sign wear</th>\n",
       "      <td>1.681844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rope knot inside jug</th>\n",
       "      <td>1.681844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ping pong ball inside</th>\n",
       "      <td>1.681844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>classtiny length minsbr spani</th>\n",
       "      <td>1.645614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  score\n",
       "span classtiny length minsbr   4.137699\n",
       "keep busy long time            2.407807\n",
       "keep busy hour try             2.350411\n",
       "year old border collie         2.192329\n",
       "get food keep entertain        2.000000\n",
       "year use every day             2.000000\n",
       "doesnt show sign wear          1.681844\n",
       "rope knot inside jug           1.681844\n",
       "ping pong ball inside          1.681844\n",
       "classtiny length minsbr spani  1.645614"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# top n grams for good reviews of products\n",
    "good_scores = {}\n",
    "for p in products:\n",
    "    corpus = list(good_dict[p][\"Text Lemmatized\"].values)\n",
    "\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "\n",
    "    tf_idf = tf_idf.sum(axis=1)\n",
    "    score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "    score.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    good_scores[p] = score\n",
    "    print(f'Top 10 n grams for good reviews of product {p}: \\n')\n",
    "    display(score.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abKersBjMF1k"
   },
   "source": [
    "Top reasons for a good review:\n",
    "* Greenie dog treats: dogs enjoy the treat and it helps keep their teeth clean\n",
    "* Popchips potato chips: customers enjoy the taste and appreciate that it has a higher nutritional value than other snacks\n",
    "* Quaker soft baked oatmeal cookies: reviewers enjoy the soft shewy taste and find the cookies to be like home baked ones. Customers also find the individual packaging convenient\n",
    "* Tug a jug meal dispensing dog toy: in good reviews customers mentioned that the product kept their dogs occupied and entertained while also dispensing delicious treat for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1649608560871,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "IsbR2BmK1-X0",
    "outputId": "143df0b9-6ed6-4fd1-8c2b-f1c2f8198dee",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for poor reviews of product Greenie: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>poor thing death maybe</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pup teeth agressive chewer</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>remain greenies worm please</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product contains toxic ingriedant</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>propylne glycol list treat</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>puppy even grow tiny</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purposebr puppy even grow</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pooch love veggi dental</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pomeranian due bite chunk</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>receive regular size need</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      score\n",
       "poor thing death maybe             1.000000\n",
       "pup teeth agressive chewer         0.707107\n",
       "remain greenies worm please        0.707107\n",
       "product contains toxic ingriedant  0.707107\n",
       "propylne glycol list treat         0.707107\n",
       "puppy even grow tiny               0.707107\n",
       "purposebr puppy even grow          0.707107\n",
       "pooch love veggi dental            0.707107\n",
       "pomeranian due bite chunk          0.707107\n",
       "receive regular size need          0.707107"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for poor reviews of product Popchips: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>popchips however like flavor</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salt vinegar wish could</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popchips box order amazon</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>purchase past enjoyed whole</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quite disappointed best nothing</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salt vinegar horrible blecht</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pop chip satisfy good</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>popchips month nowlove flavor</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salt pepper flavor fantastic</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>realised ate later stink</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 score\n",
       "popchips however like flavor       1.0\n",
       "salt vinegar wish could            1.0\n",
       "popchips box order amazon          1.0\n",
       "purchase past enjoyed whole        1.0\n",
       "quite disappointed best nothing    1.0\n",
       "salt vinegar horrible blecht       1.0\n",
       "pop chip satisfy good              1.0\n",
       "popchips month nowlove flavor      1.0\n",
       "salt pepper flavor fantastic       1.0\n",
       "realised ate later stink           1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for poor reviews of product Quaker_Oatmeal: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>toddler love literally eat</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>momvoxbox influensteri loverhowever favsmy</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toss bite kid throw</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>much toss bite kid</th>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>obviously sugar seem bit</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>serve family obviously sugar</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolled oat instead oat</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>would serve family obviously</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oat flour crumbly mealy</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oat instead oat flour</th>\n",
       "      <td>0.577350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               score\n",
       "toddler love literally eat                  0.707107\n",
       "momvoxbox influensteri loverhowever favsmy  0.707107\n",
       "toss bite kid throw                         0.707107\n",
       "much toss bite kid                          0.707107\n",
       "obviously sugar seem bit                    0.577350\n",
       "serve family obviously sugar                0.577350\n",
       "rolled oat instead oat                      0.577350\n",
       "would serve family obviously                0.577350\n",
       "oat flour crumbly mealy                     0.577350\n",
       "oat instead oat flour                       0.577350"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 n grams for poor reviews of product Tug_a_jug: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>matter hard try food</th>\n",
       "      <td>1.679176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month old lab puppy</th>\n",
       "      <td>1.679176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard wood floor loud</th>\n",
       "      <td>1.561844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wish could get money</th>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>could get money back</th>\n",
       "      <td>1.414214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doodle chew rope within</th>\n",
       "      <td>1.268951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>golden doodle chew rope</th>\n",
       "      <td>1.268951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month old yellow lab</th>\n",
       "      <td>1.241020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>think would great toy</th>\n",
       "      <td>1.226700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toy last ten minute</th>\n",
       "      <td>1.018738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            score\n",
       "matter hard try food     1.679176\n",
       "month old lab puppy      1.679176\n",
       "hard wood floor loud     1.561844\n",
       "wish could get money     1.414214\n",
       "could get money back     1.414214\n",
       "doodle chew rope within  1.268951\n",
       "golden doodle chew rope  1.268951\n",
       "month old yellow lab     1.241020\n",
       "think would great toy    1.226700\n",
       "toy last ten minute      1.018738"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# top n grams for poor reviews of products\n",
    "poor_scores = {}\n",
    "for p in products:\n",
    "    corpus = list(poor_dict[p][\"Text Lemmatized\"].values)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "\n",
    "    tf_idf = tf_idf.sum(axis=1)\n",
    "    score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "    score.sort_values(by=\"score\", ascending=False, inplace=True)\n",
    "    poor_scores[p] = score\n",
    "    print(f'Top 10 n grams for poor reviews of product {p}: \\n')\n",
    "    display(score.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu3mIEyWMDzf"
   },
   "source": [
    "Top reasons for a poor review and customer dissatifaction:\n",
    "* Greenie dog treats: customers are concerned that the treats has toxic ingredients and can also be fatal to their pets\n",
    "* Popchips potato chips: customers pointed out that the texture of the chips isn't great. Also, the chips do not taste great with some customers even saying that they taste like chemical and contain preservatives\n",
    "* Quaker soft baked oatmeal cookies: customers do not appreciate the texture, along with the fact that it is made with oat flour and even find it to be crumbly. Some customers also complained about the high sugar content\n",
    "* Tug a jug meal dispensing dog toy: customers mentioned that their dogs chewed through the rope and that the product was not very durable and long lasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF and its limitations:\n",
    "* TF-IDF findings help us determine common themes across text, however, they are not completely helpful in determine the exact meaning of the text. Rather, it evaluates the tendency of pre-processed words to be found together in n-grams.\n",
    "* TF-IDF cancels out the incapabilities of Bag of Words technique and highlights each word's relevance in the entire document\n",
    "* Since it is based on the BOW model, it does not capture semantics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity and word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "      <th>Product</th>\n",
       "      <th>good_review</th>\n",
       "      <th>Text Lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>12 year old sheltie chronic brochotitis meds t...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>12 year old sheltie chronic brochotitis med th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>genuine greenies product not knockoff dogs lov...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>genuine greenies product not knockoff dog love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>dogs love greenies course doggies dont bought ...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>dog love greenies course doggy dont buy dashch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>say dogs love greenies begg time always sit cu...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>say dog love greenies begg time always sit cup...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>review box greenies lite dog package came quic...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>review box greenies lite dog package come quic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>highly recommend chews dog exactly say freshen...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>highly recommend chew dog exactly say freshen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>always used greenies dogs seniors wonderful el...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>always use greenies dog senior wonderful elder...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>youve got hard chewers like one dogs things wo...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>youve get hard chewer like one dog thing wont ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>tried greenies dog 8 years ago wont eat tried ...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>tried greenies dog 8 year ago wont eat tried d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>fabulous treat little cavalier king charles lo...</td>\n",
       "      <td>Greenie</td>\n",
       "      <td>2</td>\n",
       "      <td>fabulous treat little cavalier king charles lo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text  Product  \\\n",
       "0      5  12 year old sheltie chronic brochotitis meds t...  Greenie   \n",
       "1      5  genuine greenies product not knockoff dogs lov...  Greenie   \n",
       "2      5  dogs love greenies course doggies dont bought ...  Greenie   \n",
       "3      5  say dogs love greenies begg time always sit cu...  Greenie   \n",
       "4      5  review box greenies lite dog package came quic...  Greenie   \n",
       "5      5  highly recommend chews dog exactly say freshen...  Greenie   \n",
       "6      5  always used greenies dogs seniors wonderful el...  Greenie   \n",
       "7      5  youve got hard chewers like one dogs things wo...  Greenie   \n",
       "8      5  tried greenies dog 8 years ago wont eat tried ...  Greenie   \n",
       "9      5  fabulous treat little cavalier king charles lo...  Greenie   \n",
       "\n",
       "   good_review                                    Text Lemmatized  \n",
       "0            2  12 year old sheltie chronic brochotitis med th...  \n",
       "1            2  genuine greenies product not knockoff dog love...  \n",
       "2            2  dog love greenies course doggy dont buy dashch...  \n",
       "3            2  say dog love greenies begg time always sit cup...  \n",
       "4            2  review box greenies lite dog package come quic...  \n",
       "5            2  highly recommend chew dog exactly say freshen ...  \n",
       "6            2  always use greenies dog senior wonderful elder...  \n",
       "7            2  youve get hard chewer like one dog thing wont ...  \n",
       "8            2  tried greenies dog 8 year ago wont eat tried d...  \n",
       "9            2  fabulous treat little cavalier king charles lo...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1),\n",
    "                             token_pattern=r'\\b[a-zA-Z_]{3,}\\b',\n",
    "                             max_df=0.2, max_features=200, stop_words=stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(data[\"Text Lemmatized\"].values)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "terms = vectorizer.get_feature_names()\n",
    "tf_idf = pd.DataFrame(X.toarray().transpose(), index=terms)\n",
    "vector = X.toarray()\n",
    "# tf_idf = tf_idf.sum(axis=1)\n",
    "# score = pd.DataFrame(tf_idf, columns=[\"score\"])\n",
    "# score.sort_values(by=\"score\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy import dot\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(A, B):\n",
    "    numerator = dot(A, B)\n",
    "    denominator = norm(A) * norm(B)\n",
    "    return numerator / denominator # remember, you take 1 - the distance to get the distance\n",
    "\n",
    "def cosine_distance(A,B):\n",
    "    return 1 - cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_score_list=[]\n",
    "c_score_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,r1 in enumerate(vector):\n",
    "    e_max_score = 999999\n",
    "    e_idx=-1\n",
    "    c_max_score = -1\n",
    "    c_idx=-1\n",
    "    if sum(vector[i]) == 0:\n",
    "        continue\n",
    "    for j in range(i+1,len(vector)):\n",
    "        if sum(vector[j]) == 0:\n",
    "            continue\n",
    "        c_score=1-cosine(vector[i], vector[j])\n",
    "        X = [vector[i], vector[j]]\n",
    "        e_score=euclidean_distances(X).ravel()[1]\n",
    "        if c_score > c_max_score:\n",
    "            c_max_score = c_score\n",
    "            c_idx=j\n",
    "        if e_score<e_max_score:\n",
    "            e_max_score = e_score\n",
    "            e_idx=j\n",
    "    e_score_list.append([i,e_idx,e_max_score])\n",
    "    c_score_list.append([i,c_idx,c_max_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 316, 1],\n",
       " [2098, 2564, 1],\n",
       " [469, 473, 0.9302869079538333],\n",
       " [1512, 1560, 0.9167575405018016],\n",
       " [1774, 2317, 0.901006456333981]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(c_score_list, key=lambda x: x[2], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Cosine Similarity, the similar reviews are:\n",
      "Review 1: dogs love greenies course doggies dont bought dashchund minpin perfect great price great product could ask\n",
      "Review 2: dogs love greenies course doggies dont bought dashchund minpin perfect great price great product could ask\n",
      " & \n",
      "According to Cosine Similarity, the similar reviews are:\n",
      "Review 1: recieved gift box exxited try soft chewy great w mik\n",
      "Review 2: like soft chewy es youll like youre going buy one box go fast\n"
     ]
    }
   ],
   "source": [
    "print('According to Cosine Similarity, the similar reviews are:')\n",
    "print(f'Review 1: {data.iloc[2][\"Text\"]}')\n",
    "print(f'Review 2: {data.iloc[316][\"Text\"]}')\n",
    "print(' & ')\n",
    "print('According to Cosine Similarity, the similar reviews are:')\n",
    "print(f'Review 1: {data.iloc[2098][\"Text\"]}')\n",
    "print(f'Review 2: {data.iloc[2564][\"Text\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 316, 0.0],\n",
       " [2098, 2564, 0.0],\n",
       " [469, 473, 0.37339815759097356],\n",
       " [1512, 1560, 0.408025635219648],\n",
       " [1774, 2317, 0.44495739945756374]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(e_score_list, key=lambda x: x[2])[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to Euclidean Distance, the similar reviews are:\n",
      "Review 1: dog love greenies course doggy dont buy dashchund minpin perfect great price great product could ask\n",
      "Review 2: dog love greenies course doggy dont buy dashchund minpin perfect great price great product could ask\n",
      " & \n",
      "According to Euclidean Distance, the similar reviews are:\n",
      "Review 1: recieved gift box exxited try soft chewy great w mik\n",
      "Review 2: like soft chewy es youll like youre going buy one box go fast\n"
     ]
    }
   ],
   "source": [
    "print('According to Euclidean Distance, the similar reviews are:')\n",
    "print(f'Review 1: {data.iloc[2][\"Text Lemmatized\"]}')\n",
    "print(f'Review 2: {data.iloc[316][\"Text Lemmatized\"]}')\n",
    "print(' & ')\n",
    "print('According to Euclidean Distance, the similar reviews are:')\n",
    "print(f'Review 1: {data.iloc[2098][\"Text\"]}')\n",
    "print(f'Review 2: {data.iloc[2564][\"Text\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 413,
     "status": "aborted",
     "timestamp": 1649551653365,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "8bneYvWiX6n4"
   },
   "source": [
    "### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "aborted",
     "timestamp": 1649551653367,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "gXeMr5GIX5z-"
   },
   "outputs": [],
   "source": [
    "documents = [\n",
    "    (\"Love this movie. Canâ€™t wait!\", \"Intent to Buy\"),\n",
    "    (\"I want to see this movie so bad.\", \"Intent to Buy\"),\n",
    "    (\"This movie looks amazing\", \"Intent to Buy\"),\n",
    "    (\"Looks bad.\",\"No Intent to Buy\"),\n",
    "    (\"Hard pass to see this bad movie.\",'No Intent to Buy'),\n",
    "    ('So boring!','No Intent to Buy')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 417,
     "status": "aborted",
     "timestamp": 1649551653370,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "jnpfLp3SX54N"
   },
   "outputs": [],
   "source": [
    "stopwords = set(['to','this'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "executionInfo": {
     "elapsed": 419,
     "status": "aborted",
     "timestamp": 1649551653372,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "ZTQCAaKtX57p"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 420,
     "status": "aborted",
     "timestamp": 1649551653374,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "u7EIZL6LYGZs"
   },
   "outputs": [],
   "source": [
    "corpus = set()\n",
    "\n",
    "# Build corpus\n",
    "for document in documents:\n",
    "    text = document[0]\n",
    "    class_value = document[1]\n",
    "    for word in text.split():\n",
    "        word=word.lower()\n",
    "        word=re.sub('[^a-zA-Z0-9 \\n]','', word)\n",
    "        word=pattern.sub('', word)\n",
    "        corpus.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "executionInfo": {
     "elapsed": 422,
     "status": "aborted",
     "timestamp": 1649551653376,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "piH8tm6VYG3m"
   },
   "outputs": [],
   "source": [
    "corpus = corpus -stopwords\n",
    "corpus=set(list(corpus)[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg-16j6BYXj3"
   },
   "source": [
    "Priors:<br>\n",
    "- P(Y='Intent to Buy') = 1/2 (3 out of 6 documents)\n",
    "- P(Y=' No Intent to Buy') = 1/2 (3 out of 6 documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 424,
     "status": "aborted",
     "timestamp": 1649551653379,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "LePB2GReYG-B"
   },
   "outputs": [],
   "source": [
    "conditional_probabilities = pd.DataFrame(index=list(corpus), \n",
    "                                         columns=[\"likelihood_given_Intent\", \"likelihood_given_No_Intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "aborted",
     "timestamp": 1649551653381,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "4YiKQeynYG65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Love this movie. Canâ€™t wait!', 'Intent to Buy')\n",
      "No intent to buy documents: 0\n",
      "Intent to buy documents: 1 \n",
      "\n",
      "\n",
      "('I want to see this movie so bad.', 'Intent to Buy')\n",
      "No intent to buy documents: 0\n",
      "Intent to buy documents: 2 \n",
      "\n",
      "\n",
      "('This movie looks amazing', 'Intent to Buy')\n",
      "No intent to buy documents: 0\n",
      "Intent to buy documents: 3 \n",
      "\n",
      "\n",
      "('Looks bad.', 'No Intent to Buy')\n",
      "No intent to buy documents: 1\n",
      "Intent to buy documents: 3 \n",
      "\n",
      "\n",
      "('Hard pass to see this bad movie.', 'No Intent to Buy')\n",
      "No intent to buy documents: 2\n",
      "Intent to buy documents: 3 \n",
      "\n",
      "\n",
      "('So boring!', 'No Intent to Buy')\n",
      "No intent to buy documents: 3\n",
      "Intent to buy documents: 3 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "intent_documents = 0\n",
    "no_intent_documents = 0\n",
    "for document in documents:\n",
    "    if document[1] == \"Intent to Buy\":\n",
    "        intent_documents += 1\n",
    "    else:\n",
    "        no_intent_documents += 1\n",
    "\n",
    "    print(f\"{document}\")\n",
    "    print(f\"No intent to buy documents: {no_intent_documents}\")\n",
    "    print(f\"Intent to buy documents: {intent_documents} \\n\\n\")\n",
    "    \n",
    "p_intent = intent_documents / (no_intent_documents + intent_documents)\n",
    "p_no_intent= no_intent_documents / (no_intent_documents + intent_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 428,
     "status": "aborted",
     "timestamp": 1649551653383,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "mB2GAnLiYHBT"
   },
   "outputs": [],
   "source": [
    "### Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 429,
     "status": "aborted",
     "timestamp": 1649551653385,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "9ODMdug_YHEp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For word see, 1 intent out of 3 intent documents.\n",
      "For word see, 1 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word cant, 1 intent out of 3 intent documents.\n",
      "For word cant, 0 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word bad, 1 intent out of 3 intent documents.\n",
      "For word bad, 2 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word pass, 0 intent out of 3 intent documents.\n",
      "For word pass, 1 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word looks, 1 intent out of 3 intent documents.\n",
      "For word looks, 1 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word love, 1 intent out of 3 intent documents.\n",
      "For word love, 0 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word movie, 3 intent out of 3 intent documents.\n",
      "For word movie, 1 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word hard, 0 intent out of 3 intent documents.\n",
      "For word hard, 1 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word want, 1 intent out of 3 intent documents.\n",
      "For word want, 0 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word boring, 0 intent out of 3 intent documents.\n",
      "For word boring, 1 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word wait, 1 intent out of 3 intent documents.\n",
      "For word wait, 0 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word amazing, 1 intent out of 3 intent documents.\n",
      "For word amazing, 0 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word so, 1 intent out of 3 intent documents.\n",
      "For word so, 1 no_intent out of 3 no_intent documents.\n",
      "\n",
      "For word i, 1 intent out of 3 intent documents.\n",
      "For word i, 0 no_intent out of 3 no_intent documents.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for word in corpus:\n",
    "    \n",
    "    intent_documents_with_word = 0\n",
    "    no_intent_documents_with_word = 0\n",
    "    \n",
    "    for document in documents:\n",
    "        document=list(document)\n",
    "        document[0]=document[0].lower()\n",
    "        document[0]=re.sub('[^a-zA-Z0-9 \\n]','', document[0])\n",
    "        document[0]=pattern.sub('', document[0])\n",
    "        #document=set(document)\n",
    "        #print(document[1])\n",
    "        document_class = list(document[1])\n",
    "        if word in document[0].split():\n",
    "            if document[1] == 'Intent to Buy':\n",
    "                intent_documents_with_word += 1\n",
    "            else:\n",
    "                no_intent_documents_with_word += 1\n",
    "    \n",
    "    print(f\"For word {word}, {intent_documents_with_word} intent out of {intent_documents} intent documents.\")\n",
    "    print(f\"For word {word}, {no_intent_documents_with_word} no_intent out of {no_intent_documents} no_intent documents.\\n\")\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_Intent\"] = intent_documents_with_word * 1.0 / intent_documents\n",
    "    conditional_probabilities.loc[word, \"likelihood_given_No_Intent\"] = no_intent_documents_with_word * 1.0 / no_intent_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 431,
     "status": "aborted",
     "timestamp": 1649551653387,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "TGIf2-hdYHH5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>likelihood_given_Intent</th>\n",
       "      <th>likelihood_given_No_Intent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>see</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cant</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pass</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>looks</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>movie</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hard</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>want</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>boring</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wait</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amazing</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>so</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        likelihood_given_Intent likelihood_given_No_Intent\n",
       "see                    0.333333                   0.333333\n",
       "cant                   0.333333                        0.0\n",
       "bad                    0.333333                   0.666667\n",
       "pass                        0.0                   0.333333\n",
       "looks                  0.333333                   0.333333\n",
       "love                   0.333333                        0.0\n",
       "movie                       1.0                   0.333333\n",
       "hard                        0.0                   0.333333\n",
       "want                   0.333333                        0.0\n",
       "boring                      0.0                   0.333333\n",
       "wait                   0.333333                        0.0\n",
       "amazing                0.333333                        0.0\n",
       "so                     0.333333                   0.333333\n",
       "i                      0.333333                        0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "executionInfo": {
     "elapsed": 433,
     "status": "aborted",
     "timestamp": 1649551653389,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "x1cA83AzYHLT"
   },
   "outputs": [],
   "source": [
    "test_document = 'This looks so bad.'\n",
    "test_document=test_document.lower()\n",
    "test_document=re.sub('[^a-zA-Z0-9 \\n]','', test_document)\n",
    "test_document=test_document.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "aborted",
     "timestamp": 1649551653391,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "1-oPr4c_YHOz"
   },
   "outputs": [],
   "source": [
    "set(test_document) - set('this')\n",
    "stopwords = set(['to','this'])\n",
    "test_document = list(set(test_document) - stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 435,
     "status": "aborted",
     "timestamp": 1649551653392,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "nVgcHmpkYHSh"
   },
   "outputs": [],
   "source": [
    "test=[]\n",
    "test = \" \".join(test_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "aborted",
     "timestamp": 1649551653394,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "0xUW4GMhYHWR"
   },
   "outputs": [],
   "source": [
    "def get_likelihood(test_document, conditional_probabilities):\n",
    "    likelihood_yes = 1\n",
    "    likelihood_no = 1\n",
    "    for word in test.split():\n",
    "        likelihood_yes = likelihood_yes * conditional_probabilities.loc[word, \"likelihood_given_Intent\"]\n",
    "        likelihood_no = likelihood_no * conditional_probabilities.loc[word, \"likelihood_given_No_Intent\"]\n",
    "    \n",
    "    return likelihood_yes, likelihood_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "executionInfo": {
     "elapsed": 437,
     "status": "aborted",
     "timestamp": 1649551653395,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "hA-vCiXkYHaN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.037037037037037035, 0.07407407407407407)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_intent, likelihood_no_intent = get_likelihood(test_document, conditional_probabilities)\n",
    "likelihood_intent, likelihood_no_intent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGV8nEGNYxkL"
   },
   "source": [
    "POSTERIOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "aborted",
     "timestamp": 1649551653397,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "0bAOHXCfYHd5"
   },
   "outputs": [],
   "source": [
    "def get_posterior(likelihood_yes, likelihood_no, p_yes, p_no):\n",
    "    posterior_yes = likelihood_yes * p_yes / (likelihood_yes * p_yes + likelihood_no * p_no)\n",
    "    posterior_no = likelihood_no * p_no / (likelihood_yes * p_yes + likelihood_no * p_no)\n",
    "    return posterior_yes, posterior_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 64,
     "status": "aborted",
     "timestamp": 1649551653399,
     "user": {
      "displayName": "Falak Jain",
      "userId": "06861805370172041080"
     },
     "user_tz": 420
    },
    "id": "QEE7_WobYHhb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.6666666666666666)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_posterior(likelihood_intent, likelihood_no_intent, p_intent, p_no_intent)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Sridhar_Suhas_8674826522_Falak_Jain_227430452_HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
