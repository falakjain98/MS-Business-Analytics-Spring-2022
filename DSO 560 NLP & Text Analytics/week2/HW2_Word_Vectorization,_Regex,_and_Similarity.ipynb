{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnVbGMpPcT8I"
   },
   "source": [
    "# Homework 2 (Due 6:29pm PST March 29th, 2022): Word Vectorization, Regex Practice, and Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTDtIUxxcT8K"
   },
   "source": [
    "You may work with **one other person on this assignment**. You may also work independently if you prefer.\n",
    "\n",
    "If you just want to be assigned someone to work with, message me on Slack and I will assign you a partner to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_PbR3ykcT8K"
   },
   "source": [
    "A. Using the **Amazon Toy Reviews Dataset (both positive and negative)**, **process the reviews**.\n",
    "This means you should think briefly about:\n",
    "* what stopwords to remove (should you add any custom stopwords to the set? Remove any stopwords?)\n",
    "* what regex cleaning you may need to perform (for example, are there different ways of saying `broken` that you need to account for?)\n",
    "* stemming/lemmatization (explain in your notebook why you used stemming versus lemmatization). \n",
    "\n",
    "Next, **count-vectorize the dataset**. Use the **`sklearn.feature_extraction.text.CountVectorizer`** examples from `Linear Algebra, Distance and Similarity (Completed).ipynb` and `Text Preprocessing Techniques (Completed).ipynb`.\n",
    "\n",
    "I do not want redundant features - for instance, I do not want `Christmas` and `Christ-mas` to be two distinct columns in your document-term matrix. Therefore, I'll be taking a look to make sure you've properly performed your cleaning, stopword removal, etc. to reduce the number of dimensions in your dataset. \n",
    "\n",
    "**Finally, identify the pair of reviews that are the MOST similar after performing all of these steps.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xtj2l3l6cT8K"
   },
   "source": [
    "B. **Stopwords, Stemming, Lemmatization Practice**\n",
    "\n",
    "Using the **McDonalds Negative Reviews** file from Week 1:\n",
    "* Count-vectorize the corpus. Treat each sentence as a document.\n",
    "\n",
    "How many features (dimensions) do you get when you:\n",
    "* Perform **stemming** and then count-vectorization\n",
    "* Perform **lemmatization** and then **count-vectorization**.\n",
    "* Perform **lemmatization**, remove **stopwords**, and then perform **count-vectorization**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A671uToGcT8L",
    "outputId": "8c75ea49-0b0e-4db6-9180-04546fedd833"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v9pnGikQHI4M"
   },
   "source": [
    "## A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "XUv3qpAhHMs3"
   },
   "outputs": [],
   "source": [
    "# remove the punctuation marks before stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "QEj0oe2h1Cml",
    "outputId": "b85b7f14-841a-4100-f77a-a53df7908288"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Excellent!!!</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Great quality wooden track (better than some o...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my daughter loved it and i liked the price and...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Great item. Pictures pop thru and add detail a...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I was pleased with the product.</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Type\n",
       "0                                       Excellent!!!  Good\n",
       "1  Great quality wooden track (better than some o...  Good\n",
       "2  my daughter loved it and i liked the price and...  Good\n",
       "3  Great item. Pictures pop thru and add detail a...  Good\n",
       "4                    I was pleased with the product.  Good"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading\n",
    "amzn_df1 = pd.read_csv('../datasets/good_amazon_toy_reviews.txt',header = None)\n",
    "amzn_df1['Type'] = 'Good'\n",
    "amzn_df2 = pd.read_csv('../datasets/poor_amazon_toy_reviews.txt', header = None)\n",
    "amzn_df2['Type'] = 'Poor'\n",
    "# Concatenating Good and Poor Reviews\n",
    "amzn_df = pd.concat([amzn_df1,amzn_df2],axis = 0)\n",
    "amzn_df = amzn_df.replace(np.nan, '', regex=True)\n",
    "# Quality Check\n",
    "assert amzn_df.shape[0] == amzn_df1.shape[0] + amzn_df2.shape[0]\n",
    "# Deleting redundant variables\n",
    "del amzn_df1, amzn_df2\n",
    "# Renaming column\n",
    "amzn_df.rename(columns = {0:'Review'},inplace = True)\n",
    "amzn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpSdB5X5636X",
    "outputId": "905f09e0-4a87-4122-cb31-354f8b41fad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'won', 'itself', \"won't\", 'too', \"aren't\", 'if', 'him', 'the', 'wouldn', \"needn't\", 'very', 'its', 'now', 'shan', 'which', 's', 'after', 'whom', 'by', 'only', 'into', 'off', 'while', 'we', 'not', 'it', 'out', 'an', \"shan't\", \"mightn't\", \"shouldn't\", \"it's\", 'few', 'hers', 'over', 'o', 'hadn', 'theirs', \"don't\", 'couldn', 'just', 'some', 'their', 'be', 'as', 'were', 've', 'i', 'why', 'ma', \"wasn't\", \"you'd\", 'so', 'doing', 'me', 'with', 'there', 'against', 'when', 'those', 'they', \"you're\", 'was', 'own', 'each', 't', 'our', 'how', 'between', 'all', 'here', 'y', 'you', 'further', 'should', 'didn', 'will', 'in', 'these', \"hasn't\", 'yours', 'herself', 'm', 'at', 'yourself', \"she's\", 'through', 'doesn', 'mustn', \"you'll\", 'during', 'who', 'below', 'did', \"isn't\", 'both', 'nor', 'from', 'themselves', 'and', \"weren't\", 'where', 'her', 'same', 'under', 'then', 'my', 'ourselves', \"wouldn't\", 'aren', 'before', 'no', 'don', 'd', 'down', 'what', 'do', \"mustn't\", 're', 'himself', 'to', 'a', 'hasn', 'because', 'your', \"hadn't\", 'his', 'has', 'any', 'isn', \"didn't\", \"haven't\", 'does', 'but', 'have', 'that', 'more', 'being', 'had', 'ain', 'been', 'mightn', 'of', \"you've\", 'for', 'is', 'once', 'up', 'shouldn', 'weren', 'again', 'them', 'she', 'myself', 'ours', \"should've\", 'yourselves', \"couldn't\", 'above', 'needn', 'can', 'are', 'am', 'about', 'other', 'this', 'wasn', 'until', \"that'll\", 'on', 'or', 'having', 'most', 'such', 'll', \"doesn't\", 'haven', 'than', 'he'}\n"
     ]
    }
   ],
   "source": [
    "# printing all the stopwords in nltk stopwords variable\n",
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHh7_CKx8m6O"
   },
   "source": [
    "Stopwords to remove from nltk list:\n",
    "- \"not\",\"doesn't\",\"didn't\": signifies negativity\n",
    "- \"very\",\"too\": signifies emphasis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOIhMKz8HvoJ"
   },
   "source": [
    "Words to be cleaned using regex cleaning:\n",
    "- \"\\\\\\&#34;\": probably an encoding error\n",
    "- \"\\\\\\<br />\": probably encoding error\n",
    "- \"pre-school\",\"preschool\"\n",
    "- \"tkx\",\"thnx\",\"thanks\",\"ty\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "J-WRZOtLxe3g"
   },
   "outputs": [],
   "source": [
    "# stop words to remove from default stopwords list\n",
    "stops= [\"not\",\"doesn't\",\"didn't\",\"very\",\"too\"]\n",
    "# creating final list of stopwords\n",
    "my_stopwords = list(set(stopwords.words('english'))  - set(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "wnfDa4JOD46_",
    "outputId": "84e0b78d-8887-4340-93e4-b1f25911e07d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great quality wooden track better than some ot...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my daughter loved it and i liked the price and...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great item pictures pop thru and add detail as...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i was pleased with the product</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Type\n",
       "0                                          excellent  Good\n",
       "1  great quality wooden track better than some ot...  Good\n",
       "2  my daughter loved it and i liked the price and...  Good\n",
       "3  great item pictures pop thru and add detail as...  Good\n",
       "4                     i was pleased with the product  Good"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocessing dataframe\n",
    "amzn_df['Review'] = amzn_df['Review'].str.lower()\n",
    "amzn_df['Review'] = amzn_df['Review'].str.replace(r'[\\.]{2,}', ' ',regex = True)\n",
    "amzn_df['Review'] = amzn_df['Review'].str.replace('34', ' ',regex = True)\n",
    "amzn_df['Review'] = amzn_df['Review'].str.replace(r'br\\b', ' ',regex = True)\n",
    "amzn_df['Review'] = amzn_df['Review'].str.replace(r'\\bpre-school', 'preschool',regex = True)\n",
    "amzn_df['Review'] = amzn_df['Review'].str.replace(r'\\btks\\b', 'thanks',regex = True)\n",
    "amzn_df['Review'] = amzn_df['Review'].str.replace(r'[^a-zA-Z0-9 \\n]', '',regex = True)\n",
    "\n",
    "amzn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "sY344x4wDtzS",
    "outputId": "3b6d1d59-eda8-4846-8745-1b9b7e37af4f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excellent</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great quality wooden track better others tried...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daughter loved liked price came rather shoppin...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great item pictures pop thru add detail painte...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pleased product</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Type\n",
       "0                                          excellent  Good\n",
       "1  great quality wooden track better others tried...  Good\n",
       "2  daughter loved liked price came rather shoppin...  Good\n",
       "3  great item pictures pop thru add detail painte...  Good\n",
       "4                                    pleased product  Good"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing all stopwords from dataframe\n",
    "amzn_df['Review'] = amzn_df['Review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (my_stopwords)]))\n",
    "amzn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FOt2PCSsayYu",
    "outputId": "06d6ef85-f862-41da-bd75-417b3fbdde8f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pleased product'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amzn_df.iloc[4,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "TJpgnDw5iC3e"
   },
   "outputs": [],
   "source": [
    "# count_word function\n",
    "def count_words(line, delimiter=\" \"):\n",
    "    words = Counter() # instantiate a Counter object called words\n",
    "    for word in line.split(delimiter):\n",
    "            words[word] += 1 # increment count for word\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "o8MClo62dws8",
    "outputId": "db6b3d72-56be-4179-9bfd-8da36444d823"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>excel</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>great qualiti wooden track better other tri pe...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>daughter love like price came rather shop ton ...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>great item pictur pop thru add detail paint dr...</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pleas product</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Type\n",
       "0                                              excel  Good\n",
       "1  great qualiti wooden track better other tri pe...  Good\n",
       "2  daughter love like price came rather shop ton ...  Good\n",
       "3  great item pictur pop thru add detail paint dr...  Good\n",
       "4                                      pleas product  Good"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# performing stemming\n",
    "stemmer = PorterStemmer()\n",
    "amzn_df['Review'] = amzn_df['Review'].apply(lambda x: \" \".join([stemmer.stem(y) for y in count_words(x)]))\n",
    "amzn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "UuNaxx8pfeAw"
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 29.7 GiB for an array with shape (114884, 34677) and data type int64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2732/3609313170.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreviews\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcorpus_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcorpus_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1029\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0morder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m             \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cf'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Output array must be C or F contiguous'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1202\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 29.7 GiB for an array with shape (114884, 34677) and data type int64"
     ]
    }
   ],
   "source": [
    "# performing count vectorization\n",
    "reviews: pd.Series = amzn_df[\"Review\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(reviews) \n",
    "X = X.toarray()\n",
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OcOqpu1acT8L"
   },
   "source": [
    "## B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uJn_D7zYcT8M"
   },
   "outputs": [],
   "source": [
    "stops= ['mcdonalds','McD',\"mcdonald's\",'mcds','mcd']\n",
    "stopwords = list(set(stopwords.words('english'))  - set([\"through\"]) | set(stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qfpDKtApcT8M"
   },
   "outputs": [],
   "source": [
    "mcd_df = pd.read_csv('../datasets/mcdonalds-yelp-negative-reviews.csv',encoding = 'latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "M6ZTBAFHcT8M"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "B49nSs9hcT8M"
   },
   "outputs": [],
   "source": [
    "mcd_df = mcd_df.replace(np.nan, '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "I93y1vmLcT8M",
    "outputId": "7260b0c0-847a-4f21-a38f-9c4ad424ae40"
   },
   "outputs": [],
   "source": [
    "pattern = re.compile(r'\\b(' + r'|'.join(stopwords) + r')\\b\\s*')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzqwGz9QeN7U"
   },
   "source": [
    "Treating each sentence as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 240
    },
    "id": "x_dts1GpePQf",
    "outputId": "6fa7fcfb-440d-4b07-9e82-d2dd7356a69e"
   },
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(columns=['_unit_id', 'city', 'sentence_id','sentence'])\n",
    "for row in mcd_df.index:\n",
    "    review=mcd_df.loc[row,'review']\n",
    "    sent_text = nltk.sent_tokenize(review)\n",
    "    for i,sentence in enumerate(sent_text):\n",
    "        rows={'_unit_id':mcd_df.loc[row,'_unit_id'],\n",
    "        'city':mcd_df.loc[row,'city'],\n",
    "        'sentence_id':i,\n",
    "        'sentence':sentence}\n",
    "        df2=df2.append(rows,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wtC2VpCeVOL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bC94srp2edi-"
   },
   "source": [
    "Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "q96mLOohefaU"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def count_words(line, delimiter=\" \"):\n",
    "    words = Counter() # instantiate a Counter object called words\n",
    "    for word in line.split(delimiter):\n",
    "            words[word] += 1 # increment count for word\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "zVIFQhkjeg_D"
   },
   "outputs": [],
   "source": [
    "def preprocessing(document):\n",
    "    document = document.lower()\n",
    "    document = re.sub('[\\.]{2,}',\" \",document)\n",
    "    document = re.sub('[^a-zA-Z0-9 \\n]', '', document)\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "vvPLy-YNeiUU"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None\n",
    "\n",
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5rzHLrYGejm0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YaRfj1GOeky2"
   },
   "source": [
    "Stemming & Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "-R_BRKJLenLM"
   },
   "outputs": [],
   "source": [
    "for row in df2.index:\n",
    "    document=df2.loc[row,'sentence']\n",
    "    document=preprocessing(document)\n",
    "    counter=count_words(document)\n",
    "    stemmed_sentence=[]\n",
    "    for word in counter:\n",
    "        stemmed_sentence.append(stemmer.stem(word))\n",
    "        newsentence=\" \".join(stemmed_sentence)\n",
    "    df2.loc[row,'sentence']=newsentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "KbClR6tQeq9m"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_unit_id</th>\n",
       "      <th>city</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>0</td>\n",
       "      <td>im not a huge mcd lover but ive been to better...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>1</td>\n",
       "      <td>thi is by far the worst one ive ever been too</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>2</td>\n",
       "      <td>it filthi insid and if you get drive through t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>679455653</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>3</td>\n",
       "      <td>the staff is terribl unfriendli and nobodi see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>679455654</td>\n",
       "      <td>Atlanta</td>\n",
       "      <td>0</td>\n",
       "      <td>terribl custom servic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    _unit_id     city sentence_id  \\\n",
       "0  679455653  Atlanta           0   \n",
       "1  679455653  Atlanta           1   \n",
       "2  679455653  Atlanta           2   \n",
       "3  679455653  Atlanta           3   \n",
       "4  679455654  Atlanta           0   \n",
       "\n",
       "                                            sentence  \n",
       "0  im not a huge mcd lover but ive been to better...  \n",
       "1      thi is by far the worst one ive ever been too  \n",
       "2  it filthi insid and if you get drive through t...  \n",
       "3  the staff is terribl unfriendli and nobodi see...  \n",
       "4                              terribl custom servic  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "MXWXT4Z2eq-Y"
   },
   "outputs": [],
   "source": [
    "df2_reviews:pd.Series=df2['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7BzHnG9YerAy"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df2_reviews) \n",
    "X = X.toarray()\n",
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114884"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.Series(amzn_df['Review']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJrXcLwgerDI"
   },
   "outputs": [],
   "source": [
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sEnuzFAiTBn"
   },
   "source": [
    "Lematization & count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxxOt7j4erFP"
   },
   "outputs": [],
   "source": [
    "df3=pd.DataFrame(columns=['_unit_id', 'city', 'sentence_id','sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IjaKzbOHerHb"
   },
   "outputs": [],
   "source": [
    "for row in mcd_df.index:\n",
    "    review=mcd_df.loc[row,'review']\n",
    "    sent_text = nltk.sent_tokenize(review)\n",
    "    for i,sentence in enumerate(sent_text):\n",
    "        rows={'_unit_id':mcd_df.loc[row,'_unit_id'],\n",
    "        'city':mcd_df.loc[row,'city'],\n",
    "        'sentence_id':i,\n",
    "        'sentence':sentence}\n",
    "        df3=df3.append(rows,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2OtmlZRerJz"
   },
   "outputs": [],
   "source": [
    "for row in df3.index:\n",
    "    document=df3.loc[row,'sentence']\n",
    "    document=preprocessing(document)\n",
    "    counter=count_words(document)\n",
    "    sentence = lemmatize_sentence(document)\n",
    "    df3.loc[row,'sentence']=sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKfBH_kNerMZ"
   },
   "outputs": [],
   "source": [
    "df3_reviews:pd.Series=df3['sentence']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df3_reviews) \n",
    "X = X.toarray()\n",
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6an77X1ij4G"
   },
   "source": [
    "Lemmatization & count vectorization gives 8105 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3uAa0dTiodM"
   },
   "source": [
    "### Perform lemmatization, remove stopwords and then count vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ew10diBuijM9"
   },
   "outputs": [],
   "source": [
    "for row in df3.index:\n",
    "    sentence=df3.loc[row,'sentence']\n",
    "    sentence=pattern.sub('', sentence)\n",
    "    df3.loc[row,'sentence']=sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeJgpfblijPJ"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df3_reviews) \n",
    "X = X.toarray()\n",
    "corpus_df = pd.DataFrame(X, columns=vectorizer.get_feature_names())\n",
    "corpus_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6TZZqkMlmN7_"
   },
   "source": [
    "### There are 7968 features after lemmatization -> remove stopwords -> count vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWCUYpaGibL_"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "6TZZqkMlmN7_"
   ],
   "name": "HW2 Word Vectorization, Regex, and Similarity.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
